{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9243169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ============================================\n",
    "# 0. CONFIG\n",
    "# ============================================\n",
    "\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test_for_participants.csv\"\n",
    "SAMPLE_SUB_PATH = \"sample_submission.csv\"\n",
    "OUTPUT_SUB_PATH = \"xgb_ensemble_top3_residual_submission.csv\"\n",
    "\n",
    "TARGET_COL = \"target\"\n",
    "DATE_COL = \"delivery_start\"\n",
    "\n",
    "# Rolling CV config: 7 months train, 1 month val\n",
    "TRAIN_MONTHS = 7\n",
    "VAL_MONTHS = 1\n",
    "\n",
    "# Hyperparameter search config\n",
    "N_ITER = 15\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Base TRAIN-ONLY trimming\n",
    "BASE_TRIM_Q_LOW = 0.01\n",
    "BASE_TRIM_Q_HIGH = 0.99\n",
    "\n",
    "# Extra TRAINING-ONLY trimming options inside base-trimmed data\n",
    "EXTRA_TAIL_TRIM_OPTIONS = [\n",
    "    (0.00, 1.00),# no extra trimming\n",
    "    #(0.01,0.99),\n",
    "    #(0.02,0.98)\n",
    "    \n",
    "]\n",
    "\n",
    "# Logging config for debugging learning curves\n",
    "LOG_CURVES = False         \n",
    "MAX_LOG_FOLDS = 3       \n",
    "\n",
    "# ============================================\n",
    "# 1. FEATURE ENGINEERING\n",
    "# ============================================\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "\n",
    "    # --- TIME FEATURES ---\n",
    "    df[\"hour\"] = df[DATE_COL].dt.hour\n",
    "    df[\"day_of_week\"] = df[DATE_COL].dt.dayofweek\n",
    "    df[\"month\"] = df[DATE_COL].dt.month\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    df[\"year\"] = df[DATE_COL].dt.year\n",
    "\n",
    "    df[\"is_2023\"] = (df[\"year\"] == 2023).astype(int)\n",
    "    df[\"is_2024\"] = (df[\"year\"] == 2024).astype(int)\n",
    "    df[\"is_2025\"] = (df[\"year\"] == 2025).astype(int)\n",
    "\n",
    "    df[\"is_first_half\"] = (df[\"month\"] <= 6).astype(int)\n",
    "    df[\"is_second_half\"] = (df[\"month\"] >= 7).astype(int)\n",
    "\n",
    "    # Cyclical encodings\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "\n",
    "    df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "    df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "\n",
    "    df[\"day_of_year\"] = df[DATE_COL].dt.dayofyear\n",
    "    df[\"doy_sin\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "    df[\"doy_cos\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "\n",
    "    df[\"is_peak_hour\"] = df[\"hour\"].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "    df[\"is_night\"] = df[\"hour\"].isin([0, 1, 2, 3, 4]).astype(int)\n",
    "\n",
    "    df[\"is_winter\"] = df[\"month\"].isin([12, 1, 2]).astype(int)\n",
    "    df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8]).astype(int)\n",
    "\n",
    "    # --- WIND DIRECTION ---\n",
    "    df[\"wind_dir_rad\"] = np.deg2rad(df[\"wind_direction_80m\"])\n",
    "    df[\"wind_dir_sin\"] = np.sin(df[\"wind_dir_rad\"])\n",
    "    df[\"wind_dir_cos\"] = np.cos(df[\"wind_dir_rad\"])\n",
    "\n",
    "    # --- NET LOAD & RENEWABLES ---\n",
    "    df[\"net_load\"] = df[\"load_forecast\"] - (df[\"solar_forecast\"] + df[\"wind_forecast\"])\n",
    "\n",
    "    df[\"renewable_share\"] = (df[\"solar_forecast\"] + df[\"wind_forecast\"]) / (df[\"load_forecast\"] + 1.0)\n",
    "    df[\"net_load_squared\"] = df[\"net_load\"] ** 2\n",
    "\n",
    "    # IMPORTANT: sort by market + time so ramps are chronological within each market\n",
    "    df = df.sort_values([\"market\", DATE_COL]).reset_index(drop=True)\n",
    "\n",
    "    # Ramps grouped by market\n",
    "    df[\"net_load_ramp\"] = df.groupby(\"market\")[\"net_load\"].diff()\n",
    "    df[\"load_ramp\"] = df.groupby(\"market\")[\"load_forecast\"].diff()\n",
    "    df[\"load_ramp_3h\"] = df.groupby(\"market\")[\"load_forecast\"].diff(3)\n",
    "\n",
    "    df[\"net_load_ramp\"] = df[\"net_load_ramp\"].fillna(0)\n",
    "    df[\"load_ramp\"] = df[\"load_ramp\"].fillna(0)\n",
    "    df[\"load_ramp_3h\"] = df[\"load_ramp_3h\"].fillna(0)\n",
    "\n",
    "    df[\"net_load_peak\"] = df[\"net_load\"] * df[\"is_peak_hour\"]\n",
    "    df[\"is_high_renewable\"] = (df[\"renewable_share\"] > df[\"renewable_share\"].median()).astype(int)\n",
    "\n",
    "    df[\"scarcity_score\"] = (\n",
    "        df[\"net_load\"] - df[\"net_load\"].mean()\n",
    "    ) / (df[\"net_load\"].std() + 1e-6)\n",
    "\n",
    "    # --- WIND FEATURES ---\n",
    "    def wind_power_smooth(v, c=1500):\n",
    "        v3 = v ** 3\n",
    "        return v3 / (v3 + c)\n",
    "\n",
    "    df[\"wind_power_shape\"] = wind_power_smooth(df[\"wind_speed_80m\"])\n",
    "\n",
    "    wind_scale = df[\"wind_forecast\"].mean()\n",
    "    df[\"wind_actual_proxy_MW\"] = df[\"wind_power_shape\"] * wind_scale\n",
    "\n",
    "    df[\"wind_surprise_MW\"] = df[\"wind_forecast\"] - df[\"wind_actual_proxy_MW\"]\n",
    "    df[\"wind_surprise_relative\"] = df[\"wind_surprise_MW\"] / (df[\"wind_forecast\"] + 1.0)\n",
    "    df[\"wind_surprise_sign\"] = np.sign(df[\"wind_surprise_MW\"])\n",
    "    df[\"wind_proxy_sq\"] = df[\"wind_actual_proxy_MW\"] ** 2\n",
    "    df[\"wind_peak\"] = df[\"wind_actual_proxy_MW\"] * df[\"is_peak_hour\"]\n",
    "    df[\"wind_winter\"] = df[\"wind_actual_proxy_MW\"] * df[\"is_winter\"]\n",
    "\n",
    "    # Wind ramp grouped by market\n",
    "    df[\"wind_ramp\"] = df.groupby(\"market\")[\"wind_actual_proxy_MW\"].diff()\n",
    "    df[\"wind_ramp\"] = df[\"wind_ramp\"].fillna(0)\n",
    "\n",
    "    # --- SOLAR FEATURES ---\n",
    "    df[\"solar_power_shape\"] = (\n",
    "        0.75 * df[\"direct_normal_irradiance\"] +\n",
    "        0.25 * df[\"diffuse_horizontal_irradiance\"]\n",
    "    )\n",
    "    df[\"solar_power_shape\"] /= df[\"solar_power_shape\"].max() + 1e-6\n",
    "\n",
    "    if \"air_temperature_2m\" in df.columns:\n",
    "        df[\"solar_temp_factor\"] = 1 - 0.004 * (df[\"air_temperature_2m\"] - 25)\n",
    "        df[\"solar_temp_factor\"] = df[\"solar_temp_factor\"].clip(0.8, 1.05)\n",
    "        df[\"solar_power_shape\"] *= df[\"solar_temp_factor\"]\n",
    "\n",
    "    solar_scale = df[\"solar_forecast\"].mean()\n",
    "    df[\"solar_actual_proxy_MW\"] = df[\"solar_power_shape\"] * solar_scale\n",
    "\n",
    "    df[\"solar_surprise_MW\"] = df[\"solar_forecast\"] - df[\"solar_actual_proxy_MW\"]\n",
    "    df[\"solar_surprise_relative\"] = df[\"solar_surprise_MW\"] / (df[\"solar_forecast\"] + 1.0)\n",
    "    df[\"solar_surprise_sign\"] = np.sign(df[\"solar_surprise_MW\"])\n",
    "\n",
    "    # Solar ramp grouped by market\n",
    "    df[\"solar_ramp\"] = df.groupby(\"market\")[\"solar_actual_proxy_MW\"].diff()\n",
    "    df[\"solar_ramp\"] = df[\"solar_ramp\"].fillna(0)\n",
    "\n",
    "    # System ramp\n",
    "    df[\"system_ramp\"] = df[\"load_ramp\"] - df[\"wind_ramp\"] - df[\"solar_ramp\"]\n",
    "\n",
    "    # Encode market\n",
    "    if \"market\" in df.columns:\n",
    "        df = pd.get_dummies(df, columns=[\"market\"], drop_first=True)\n",
    "\n",
    "    # Drop raw time / direction columns (keep DATE_COL)\n",
    "    drop_cols = [\"hour\", \"day_of_week\", \"month\", \"day_of_year\", \"wind_direction_80m\", \"wind_dir_rad\"]\n",
    "    drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================\n",
    "# 2. TIME-BASED ROLLING SPLITS (NO TRIM HERE)\n",
    "# ============================================\n",
    "\n",
    "def build_time_splits(dates: pd.Series, train_months=6, val_months=1):\n",
    "    dates = pd.to_datetime(dates)\n",
    "    months = dates.dt.to_period(\"M\")\n",
    "    unique_months = np.array(sorted(months.unique()))\n",
    "\n",
    "    splits = []\n",
    "    total = train_months + val_months\n",
    "\n",
    "    for start in range(len(unique_months) - total + 1):\n",
    "        train_m = unique_months[start: start + train_months]\n",
    "        val_m = unique_months[start + train_months: start + total]\n",
    "\n",
    "        train_idx = np.where(months.isin(train_m))[0]\n",
    "        val_idx = np.where(months.isin(val_m))[0]\n",
    "\n",
    "        if len(train_idx) == 0 or len(val_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        splits.append((train_idx, val_idx))\n",
    "\n",
    "    return splits\n",
    "\n",
    "# ============================================\n",
    "# 3. HYPERPARAMETER SAMPLING\n",
    "# ============================================\n",
    "\n",
    "def sample_xgb_params(rng: np.random.RandomState):\n",
    "    params = {\n",
    "        \"n_estimators\": rng.randint(1200, 7000),\n",
    "        \"max_depth\": rng.randint(4, 8),\n",
    "        \"learning_rate\": 10 ** rng.uniform(-1.5, -0.7),         # ~0.1–0.316\n",
    "        \"subsample\": rng.uniform(0.6, 0.9),\n",
    "        \"colsample_bytree\": rng.uniform(0.6, 0.9),\n",
    "        \"min_child_weight\": rng.uniform(5.0, 30.0),\n",
    "        \"gamma\": 10 ** rng.uniform(-1.5, 0),                  # ~0.03–1\n",
    "        \"reg_alpha\": 10 ** rng.uniform(-2, 1),                # 0.01–10\n",
    "        \"reg_lambda\": 10 ** rng.uniform(-1, 2),               # 0.1–100\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# ============================================\n",
    "# 4. EVALUATE ONE CONFIG (TRAIN-ONLY TRIM, RESIDUAL TARGET)\n",
    "# ============================================\n",
    "\n",
    "def evaluate_config(\n",
    "    X_all_np,\n",
    "    y_all_np,\n",
    "    splits,\n",
    "    params,\n",
    "    extra_lower_q,\n",
    "    extra_upper_q,\n",
    "    early_stopping_rounds=150,\n",
    "    baseline_col_idx=None,\n",
    "):\n",
    "    rmses = []\n",
    "    fold_id = 0\n",
    "\n",
    "    for (train_idx, val_idx) in splits:\n",
    "        fold_id += 1\n",
    "\n",
    "        X_train_full = X_all_np[train_idx]\n",
    "        y_train_full = y_all_np[train_idx]\n",
    "\n",
    "        X_val = X_all_np[val_idx]\n",
    "        y_val = y_all_np[val_idx]  # original prices\n",
    "\n",
    "        # Baseline from mh_price_mean column\n",
    "        if baseline_col_idx is None:\n",
    "            raise ValueError(\"baseline_col_idx must be provided for residual modeling\")\n",
    "\n",
    "        baseline_train = X_train_full[:, baseline_col_idx]\n",
    "        baseline_val = X_val[:, baseline_col_idx]\n",
    "\n",
    "        # Base train-only trimming (1–99%) on ORIGINAL y\n",
    "        q_low_base = np.quantile(y_train_full, BASE_TRIM_Q_LOW)\n",
    "        q_high_base = np.quantile(y_train_full, BASE_TRIM_Q_HIGH)\n",
    "        mask_base = (y_train_full >= q_low_base) & (y_train_full <= q_high_base)\n",
    "\n",
    "        y_train_base = y_train_full[mask_base]\n",
    "        X_train_base = X_train_full[mask_base]\n",
    "        baseline_train_base = baseline_train[mask_base]\n",
    "\n",
    "        # Residual target after base trim\n",
    "        y_train_base_res = y_train_base - baseline_train_base\n",
    "\n",
    "        # Extra TRAINING-ONLY trimming inside base (still based on ORIGINAL y)\n",
    "        if extra_lower_q > 0.0 or extra_upper_q < 1.0:\n",
    "            q_low_extra = np.quantile(y_train_base, extra_lower_q)\n",
    "            q_high_extra = np.quantile(y_train_base, extra_upper_q)\n",
    "            mask_extra = (y_train_base >= q_low_extra) & (y_train_base <= q_high_extra)\n",
    "\n",
    "            X_train = X_train_base[mask_extra]\n",
    "            y_train_res = y_train_base_res[mask_extra]\n",
    "        else:\n",
    "            X_train = X_train_base\n",
    "            y_train_res = y_train_base_res\n",
    "\n",
    "        if len(y_train_res) < 200:\n",
    "            continue\n",
    "\n",
    "        # Residuals on val (for early stopping metric)\n",
    "        y_val_res = y_val - baseline_val\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            **params,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train_res,\n",
    "            eval_set=[(X_train, y_train_res), (X_val, y_val_res)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Logging (now residual RMSE curves)\n",
    "        if LOG_CURVES and fold_id <= MAX_LOG_FOLDS:\n",
    "            evals = model.evals_result()\n",
    "            train_curve = evals[\"validation_0\"][\"rmse\"]\n",
    "            val_curve = evals[\"validation_1\"][\"rmse\"]\n",
    "\n",
    "            n_rounds = len(train_curve)\n",
    "            best_iter = int(np.argmin(val_curve))\n",
    "            best_train = train_curve[best_iter]\n",
    "            best_val = val_curve[best_iter]\n",
    "\n",
    "            last_train = train_curve[-1]\n",
    "            last_val = val_curve[-1]\n",
    "\n",
    "            print(f\"\\n=== Fold {fold_id} learning curves (residual target) ===\")\n",
    "            print(f\"  Rounds: {n_rounds}\")\n",
    "            print(f\"  Best iter: {best_iter} | train_rmse_res={best_train:.3f}, val_rmse_res={best_val:.3f}\")\n",
    "            print(f\"  Last iter: {n_rounds-1} | train_rmse_res={last_train:.3f}, val_rmse_res={last_val:.3f}\")\n",
    "\n",
    "            for check in [0, 10, 50, 100, 200, 500, best_iter]:\n",
    "                if 0 <= check < n_rounds:\n",
    "                    print(\n",
    "                        f\"    iter {check:4d}: \"\n",
    "                        f\"train_res={train_curve[check]:.3f}, val_res={val_curve[check]:.3f}\"\n",
    "                    )\n",
    "\n",
    "        # ---- Evaluate in ORIGINAL price space ----\n",
    "        y_pred_res = model.predict(X_val)\n",
    "        y_pred_full = y_pred_res + baseline_val\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_full))\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    if len(rmses) == 0:\n",
    "        return np.inf\n",
    "\n",
    "    return float(np.mean(rmses))\n",
    "\n",
    "# ============================================\n",
    "# 5. MAIN\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "\n",
    "    # --- Load data ---\n",
    "    train = pd.read_csv(TRAIN_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "    # --- Cut 2023 H2 (Jul–Dec 2023) from TRAIN ---\n",
    "    train[DATE_COL] = pd.to_datetime(train[DATE_COL])\n",
    "    mask_keep = ~(\n",
    "        (train[DATE_COL].dt.year == 2023) & (train[DATE_COL].dt.month >= 7)\n",
    "    )\n",
    "    print(f\"Rows before 2023 H2 cut: {len(train)}\")\n",
    "    train = train.loc[mask_keep].reset_index(drop=True)\n",
    "    print(f\"Rows after  2023 H2 cut: {len(train)}\")\n",
    "\n",
    "    # Make sure DATE_COL in test is datetime too\n",
    "    test[DATE_COL] = pd.to_datetime(test[DATE_COL])\n",
    "\n",
    "    # ============================================\n",
    "    # Market + hour price profile (from TRAIN ONLY)\n",
    "    # ============================================\n",
    "\n",
    "    train[\"profile_hour\"] = train[DATE_COL].dt.hour\n",
    "    test[\"profile_hour\"] = test[DATE_COL].dt.hour\n",
    "\n",
    "    mh_profile = (\n",
    "        train.groupby([\"market\", \"profile_hour\"])[TARGET_COL]\n",
    "        .agg([\"mean\", \"median\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"mean\": \"mh_price_mean\", \"median\": \"mh_price_median\"})\n",
    "    )\n",
    "\n",
    "    train = train.merge(mh_profile, on=[\"market\", \"profile_hour\"], how=\"left\")\n",
    "    test = test.merge(mh_profile, on=[\"market\", \"profile_hour\"], how=\"left\")\n",
    "\n",
    "    train = train.drop(columns=[\"profile_hour\"])\n",
    "    test = test.drop(columns=[\"profile_hour\"])\n",
    "\n",
    "    # --- Feature engineering ---\n",
    "    train_fe = engineer_features(train)\n",
    "    test_fe = engineer_features(test)\n",
    "\n",
    "    # --- Handle missing numeric values using train medians ---\n",
    "    numeric_cols = train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in [TARGET_COL, \"id\"]]\n",
    "\n",
    "    median_dict = train_fe[numeric_cols].median()\n",
    "    train_fe[numeric_cols] = train_fe[numeric_cols].fillna(median_dict)\n",
    "\n",
    "    for col in median_dict.index:\n",
    "        if col in test_fe.columns:\n",
    "            test_fe[col] = test_fe[col].fillna(median_dict[col])\n",
    "\n",
    "    # --- Build feature list (no global trimming here) --\n",
    "    non_feature_cols = [TARGET_COL, DATE_COL]\n",
    "    if \"delivery_end\" in train_fe.columns:\n",
    "        non_feature_cols.append(\"delivery_end\")\n",
    "    if \"id\" in train_fe.columns:\n",
    "        non_feature_cols.append(\"id\")\n",
    "\n",
    "    candidate_feature_cols = [\n",
    "        c for c in train_fe.columns if c not in non_feature_cols\n",
    "    ]\n",
    "    feature_cols = [\n",
    "        c for c in candidate_feature_cols\n",
    "        if pd.api.types.is_numeric_dtype(train_fe[c])\n",
    "    ]\n",
    "\n",
    "    # Index of baseline feature for residual modeling\n",
    "    assert \"mh_price_mean\" in feature_cols, \"mh_price_mean must be in feature_cols for residual modeling\"\n",
    "    baseline_col_idx = feature_cols.index(\"mh_price_mean\")\n",
    "\n",
    "    print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "    X_df = train_fe[feature_cols].reset_index(drop=True)\n",
    "    y_series = train_fe[TARGET_COL].reset_index(drop=True)\n",
    "\n",
    "    X_all_np = X_df.values\n",
    "    y_all_np = y_series.values\n",
    "\n",
    "    # --- Rolling splits on FULL (UNTRIMMED) data ---\n",
    "    splits = build_time_splits(train_fe[DATE_COL], TRAIN_MONTHS, VAL_MONTHS)\n",
    "    print(f\"Number of rolling splits: {len(splits)}\")\n",
    "\n",
    "    # ============================================\n",
    "    # 6. RANDOM SEARCH (TRAIN-ONLY TRIM, UNTRIMMED VAL, RESIDUAL TARGET)\n",
    "    # ============================================\n",
    "\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "    best_extra_trim = None\n",
    "\n",
    "    all_configs = []   # store (score, params, extra_trim)\n",
    "\n",
    "    for i in range(1, N_ITER + 1):\n",
    "        params = sample_xgb_params(rng)\n",
    "        extra_lower_q, extra_upper_q = EXTRA_TAIL_TRIM_OPTIONS[\n",
    "            rng.randint(len(EXTRA_TAIL_TRIM_OPTIONS))\n",
    "        ]\n",
    "\n",
    "        mean_rmse = evaluate_config(\n",
    "            X_all_np,\n",
    "            y_all_np,\n",
    "            splits,\n",
    "            params,\n",
    "            extra_lower_q,\n",
    "            extra_upper_q,\n",
    "            early_stopping_rounds=50,\n",
    "            baseline_col_idx=baseline_col_idx,\n",
    "        )\n",
    "\n",
    "        all_configs.append({\n",
    "            \"score\": mean_rmse,\n",
    "            \"params\": params,\n",
    "            \"extra_trim\": (extra_lower_q, extra_upper_q),\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"[{i}/{N_ITER}] \"\n",
    "            f\"CV RMSE={mean_rmse:.3f} | extra_trim=({extra_lower_q:.3f}, {extra_upper_q:.3f}) \"\n",
    "            f\"| n_estimators={params['n_estimators']}, \"\n",
    "            f\"max_depth={params['max_depth']}, \"\n",
    "            f\"lr={params['learning_rate']:.4f}\"\n",
    "        )\n",
    "\n",
    "        if mean_rmse < best_score:\n",
    "            best_score = mean_rmse\n",
    "            best_params = params\n",
    "            best_extra_trim = (extra_lower_q, extra_upper_q)\n",
    "            print(\n",
    "                f\"  -> New BEST: CV RMSE={best_score:.3f}, \"\n",
    "                f\"extra_trim=({extra_lower_q:.3f}, {extra_upper_q:.3f})\"\n",
    "            )\n",
    "\n",
    "    print(\"\\n===== BEST CONFIGURATION FOUND (SINGLE) =====\")\n",
    "    print(f\"Best mean CV RMSE: {best_score:.3f}\")\n",
    "    print(f\"Best extra training trim quantiles: {best_extra_trim}\")\n",
    "    print(\"Best params:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # ENSEMBLE: pick top-3 configs by CV RMSE\n",
    "    all_configs_sorted = sorted(all_configs, key=lambda c: c[\"score\"])\n",
    "    top_k = min(3, len(all_configs_sorted))\n",
    "    top_configs = all_configs_sorted[:top_k]\n",
    "\n",
    "    print(f\"\\n===== TOP {top_k} CONFIGURATIONS FOR ENSEMBLE =====\")\n",
    "    for rank, cfg in enumerate(top_configs, start=1):\n",
    "        print(f\"\\n-- Model {rank} --\")\n",
    "        print(f\"CV RMSE: {cfg['score']:.3f}\")\n",
    "        print(f\"extra_trim: {cfg['extra_trim']}\")\n",
    "        for k, v in cfg[\"params\"].items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "    # ============================================\n",
    "    # Prepare TEST FEATURES once (for all ensemble models)\n",
    "    # ============================================\n",
    "\n",
    "    test_fe = sample_sub[[\"id\"]].merge(test_fe, on=\"id\", how=\"left\")\n",
    "\n",
    "    X_test = test_fe.reindex(columns=feature_cols)\n",
    "    X_test = X_test.fillna(median_dict)\n",
    "    X_test = X_test.fillna(0.0)\n",
    "\n",
    "    baseline_test = X_test.iloc[:, baseline_col_idx].values  # mh_price_mean for test\n",
    "\n",
    "    # ============================================\n",
    "    # 7. TRAIN FINAL TOP-K MODELS & PREDICT ON TEST (RESIDUAL)\n",
    "    # ============================================\n",
    "\n",
    "    months_all = train_fe[DATE_COL].dt.to_period(\"M\")\n",
    "    final_val_month = months_all.max()\n",
    "\n",
    "    val_mask_final = months_all == final_val_month\n",
    "    train_mask_final = ~val_mask_final\n",
    "\n",
    "    X_train_full_global = X_df.loc[train_mask_final].reset_index(drop=True)\n",
    "    y_train_full_global = y_series.loc[train_mask_final].reset_index(drop=True)\n",
    "\n",
    "    X_val_final_global = X_df.loc[val_mask_final].reset_index(drop=True)\n",
    "    y_val_final_global = y_series.loc[val_mask_final].reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f\"\\nFinal split for training (shared): \"\n",
    "        f\"{train_mask_final.sum()} rows train, \"\n",
    "        f\"{val_mask_final.sum()} rows validation \"\n",
    "        f\"(val month = {final_val_month})\"\n",
    "    )\n",
    "\n",
    "    test_pred_list = []\n",
    "\n",
    "    for rank, cfg in enumerate(top_configs, start=1):\n",
    "        params_m = cfg[\"params\"]\n",
    "        extra_lower_q_m, extra_upper_q_m = cfg[\"extra_trim\"]\n",
    "\n",
    "        print(f\"\\n=== Training final model {rank} / {top_k} ===\")\n",
    "        print(f\"Using extra_trim = ({extra_lower_q_m:.3f}, {extra_upper_q_m:.3f})\")\n",
    "\n",
    "        X_train_full = X_train_full_global.copy()\n",
    "        y_train_full = y_train_full_global.copy()\n",
    "        X_val_final = X_val_final_global.copy()\n",
    "        y_val_final = y_val_final_global.copy()\n",
    "\n",
    "        baseline_train_full = X_train_full.iloc[:, baseline_col_idx].values\n",
    "        baseline_val = X_val_final.iloc[:, baseline_col_idx].values\n",
    "\n",
    "        # Base train-only trimming on original y\n",
    "        q_low_base = np.quantile(y_train_full.values, BASE_TRIM_Q_LOW)\n",
    "        q_high_base = np.quantile(y_train_full.values, BASE_TRIM_Q_HIGH)\n",
    "        mask_base_final = (y_train_full >= q_low_base) & (y_train_full <= q_high_base)\n",
    "\n",
    "        y_train_base_final = y_train_full[mask_base_final]\n",
    "        X_train_base_final = X_train_full[mask_base_final]\n",
    "        baseline_train_base = baseline_train_full[mask_base_final]\n",
    "\n",
    "        # Residual targets after base trim\n",
    "        y_train_base_res = y_train_base_final.values - baseline_train_base\n",
    "\n",
    "        if extra_lower_q_m > 0.0 or extra_upper_q_m < 1.0:\n",
    "            q_low_extra = np.quantile(y_train_base_final.values, extra_lower_q_m)\n",
    "            q_high_extra = np.quantile(y_train_base_final.values, extra_upper_q_m)\n",
    "            mask_extra_final = (y_train_base_final >= q_low_extra) & (y_train_base_final <= q_high_extra)\n",
    "\n",
    "            X_train_final = X_train_base_final[mask_extra_final]\n",
    "            y_train_final_res = y_train_base_res[mask_extra_final]\n",
    "\n",
    "            print(\n",
    "                f\"  Final training: extra trimming kept \"\n",
    "                f\"{mask_extra_final.sum()} / {len(mask_extra_final)} training rows \"\n",
    "                f\"with target in [{q_low_extra:.2f}, {q_high_extra:.2f}]\"\n",
    "            )\n",
    "        else:\n",
    "            X_train_final = X_train_base_final\n",
    "            y_train_final_res = y_train_base_res\n",
    "            print(\n",
    "                f\"  Final training: base 1–99% trimming kept \"\n",
    "                f\"{len(y_train_final_res)} / {len(y_train_full)} training rows \"\n",
    "                f\"with target in [{q_low_base:.2f}, {q_high_base:.2f}]\"\n",
    "            )\n",
    "\n",
    "        model_m = XGBRegressor(**params_m, early_stopping_rounds=50)\n",
    "\n",
    "        y_val_final_res = y_val_final.values - baseline_val\n",
    "\n",
    "        model_m.fit(\n",
    "            X_train_final.values,\n",
    "            y_train_final_res,\n",
    "            eval_set=[(X_train_final.values, y_train_final_res),\n",
    "                      (X_val_final.values, y_val_final_res)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        if hasattr(model_m, \"best_iteration\"):\n",
    "            print(f\"  Final model {rank} best_iteration (residual): {model_m.best_iteration}\")\n",
    "        else:\n",
    "            print(f\"  Final model {rank} trained (no best_iteration attribute).\")\n",
    "\n",
    "        # Predict residuals on test for this model, then add baseline\n",
    "        test_pred_res_m = model_m.predict(X_test.values)\n",
    "        test_pred_full_m = test_pred_res_m + baseline_test\n",
    "        test_pred_list.append(test_pred_full_m)\n",
    "\n",
    "    # ============================================\n",
    "    # 8. ENSEMBLE: AVERAGE TOP-K TEST PREDICTIONS\n",
    "    # ============================================\n",
    "\n",
    "    test_pred_array = np.column_stack(test_pred_list)  # shape (n_test, top_k)\n",
    "    ensemble_pred = test_pred_array.mean(axis=1)\n",
    "\n",
    "    predictions = sample_sub.copy()\n",
    "    predictions[\"target\"] = ensemble_pred.astype(float)\n",
    "\n",
    "    # ============================================\n",
    "    # 9. VALIDATE SUBMISSION FORMAT & SAVE\n",
    "    # ============================================\n",
    "\n",
    "    assert list(predictions.columns) == [\"id\", \"target\"], \"Wrong columns!\"\n",
    "    assert len(predictions) == 13098, f\"Wrong row count: {len(predictions)}\"\n",
    "    assert predictions[\"id\"].min() == 133627, \"IDs must start at 133627\"\n",
    "    assert predictions[\"id\"].max() == 146778, \"IDs must end at 146778\"\n",
    "    assert predictions[\"target\"].notna().all(), \"No NaN values allowed!\"\n",
    "    assert np.isfinite(predictions[\"target\"]).all(), \"No infinite values allowed!\"\n",
    "\n",
    "    predictions.to_csv(OUTPUT_SUB_PATH, index=False)\n",
    "    print(f\"\\n✅ Ensemble submission saved as {OUTPUT_SUB_PATH} with shape {predictions.shape}\")\n",
    "    print(f\"   (Averaged over top {top_k} residual models by CV RMSE)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
